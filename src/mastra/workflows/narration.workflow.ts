import { createWorkflow, createStep } from '@mastra/core';
import { z } from 'zod';
import { activitySchema, visionAgent } from '../agents/vision.agent';
import { narratorAgent } from '../agents/narrator.agent';
import { imageValidationTool } from '../tools/camera.tool';

// Step 1: ç”»åƒãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã‚«ãƒ¡ãƒ©ã¨ç”»é¢ï¼‰
const validateImageStep = createStep({
  id: 'validate-image',
  inputSchema: z.object({
    cameraImageBase64: z.string(),
    screenImages: z.array(z.string()),
  }),
  outputSchema: z.object({
    cameraIsValid: z.boolean(),
    screensValid: z.array(z.boolean()),
    timestamp: z.string(),
    cameraImageBase64: z.string(),
    screenImages: z.array(z.string()),
  }),
  execute: async ({ inputData }) => {
    const { cameraImageBase64, screenImages } = inputData;

    // ã‚«ãƒ¡ãƒ©ç”»åƒã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    const cameraResult = await imageValidationTool.execute({
      context: { imageBase64: cameraImageBase64 },
    });

    // ç”»é¢ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæœ€åˆã®1ã¤ã ã‘ï¼‰
    const screenResult = await imageValidationTool.execute({
      context: { imageBase64: screenImages[0] },
    });

    if (!cameraResult.isValid || !screenResult.isValid) {
      throw new Error('Invalid image data');
    }

    console.log('âœ… Camera & Screen validated');
    return {
      cameraIsValid: cameraResult.isValid,
      screensValid: [screenResult.isValid],
      timestamp: cameraResult.timestamp,
      cameraImageBase64,
      screenImages,
    };
  },
});

// Step 2: ç”»åƒåˆ†æžï¼ˆã‚«ãƒ¡ãƒ©ã¨ç”»é¢ã‚’åŒæ™‚ã«ï¼‰
const analyzeImageStep = createStep({
  id: 'analyze-image',
  inputSchema: z.object({
    cameraIsValid: z.boolean(),
    screensValid: z.array(z.boolean()),
    timestamp: z.string(),
    cameraImageBase64: z.string(),
    screenImages: z.array(z.string()),
  }),
  outputSchema: z.object({
    cameraAnalysis: activitySchema,
    screenAnalyses: z.array(z.object({
      content: z.string().describe('ç”»é¢ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®è©³ç´°'),
      activity: z.string().describe('ç”»é¢ä¸Šã§ä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼ˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã€å‹•ç”»è¦–è´ãªã©ï¼‰'),
      application: z.string().describe('ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ'),
    })),
  }),
  execute: async ({ inputData }) => {
    const { cameraImageBase64, screenImages } = inputData;

    // ã‚«ãƒ¡ãƒ©ç”»åƒã®åˆ†æž
    const cameraResult = await visionAgent.generate(
      [
        {
          role: 'user',
          content: [
            { type: 'text', text: 'Analyze the person in this camera image in detail' },
            { type: 'image', image: `data:image/jpeg;base64,${cameraImageBase64}` },
          ],
        },
      ],
      {
        structuredOutput: { schema: activitySchema },
      }
    );

    // ç”»é¢ç”»åƒã®åˆ†æžï¼ˆæœ€åˆã®1ã¤ã ã‘ï¼‰
    const screenSchema = z.object({
      content: z.string().describe('ç”»é¢ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®è©³ç´°'),
      activity: z.string().describe('ç”»é¢ä¸Šã§ä½•ã‚’ã—ã¦ã„ã‚‹ã‹ï¼ˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã€å‹•ç”»è¦–è´ãªã©ï¼‰'),
      application: z.string().describe('ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ'),
    });

    const screenResult = await visionAgent.generate(
      [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: 'Analyze this screen capture. What is displayed? What application or website is being used? What activity is being performed?',
            },
            { type: 'image', image: `data:image/jpeg;base64,${screenImages[0]}` },
          ],
        },
      ],
      {
        structuredOutput: { schema: screenSchema },
      }
    );

    console.log('âœ… Camera analyzed:', cameraResult.object);
    console.log('âœ… Screen analyzed:', screenResult.object);

    return {
      cameraAnalysis: cameraResult.object,
      screenAnalyses: [screenResult.object],
    };
  },
});

// Step 3: å®Ÿæ³æ–‡ç”Ÿæˆï¼ˆã‚«ãƒ¡ãƒ©ã¨ç”»é¢ã‚’çµ±åˆï¼‰
const generateNarrationStep = createStep({
  id: 'generate-narration',
  inputSchema: z.object({
    cameraAnalysis: activitySchema,
    screenAnalyses: z.array(z.object({
      content: z.string(),
      activity: z.string(),
      application: z.string(),
    })),
  }),
  outputSchema: z.object({ narration: z.string() }),
  execute: async ({ inputData }) => {
    const { cameraAnalysis, screenAnalyses } = inputData;

    // æœ€åˆã®ç”»é¢ï¼ˆãƒ¡ã‚¤ãƒ³ç”»é¢ï¼‰ã‚’ä½¿ç”¨
    const screenAnalysis = screenAnalyses[0];

    const prompt = `
ã€ã‚«ãƒ¡ãƒ©ï¼ˆæœ¬äººã®æ§˜å­ï¼‰ã€‘
æ´»å‹•: ${cameraAnalysis.activity}
æ°—åˆ†: ${cameraAnalysis.mood}
å§¿å‹¢: ${cameraAnalysis.posture}
è©³ç´°: ${cameraAnalysis.details}
å‘¨å›²ã®ç‰©: ${cameraAnalysis.objects?.join(', ') || 'ãªã—'}
ç’°å¢ƒ: ${cameraAnalysis.environment}

ã€ç”»é¢ï¼ˆPCä½œæ¥­å†…å®¹ï¼‰ã€‘
è¡¨ç¤ºå†…å®¹: ${screenAnalysis.content}
ä½œæ¥­å†…å®¹: ${screenAnalysis.activity}
ã‚¢ãƒ—ãƒª/ã‚µã‚¤ãƒˆ: ${screenAnalysis.application}

æœ¬äººã®æ§˜å­ã¨PCç”»é¢ã®ä¸¡æ–¹ã‚’è¦‹ã¦ã€ä»Šä½•ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã€ç†±è¡€ã‚¹ãƒãƒ¼ãƒ„å®Ÿæ³é¢¨ã«å®Ÿæ³ã—ã¦ãã ã•ã„ï¼
ç”»é¢ã§ä½•ã‚’ã—ã¦ã„ã‚‹ã‹ã€ãã®äººãŒã©ã‚“ãªçŠ¶æ…‹ã§å–ã‚Šçµ„ã‚“ã§ã„ã‚‹ã‹ã‚’2-3æ–‡ã§ã€ã‚¨ã‚­ã‚µã‚¤ãƒ†ã‚£ãƒ³ã‚°ã«ï¼
`;

    const result = await narratorAgent.generate(prompt);

    console.log('âœ… Narration generated:', result.text);
    return { narration: result.text };
  },
});

// Step 4: éŸ³å£°åˆæˆ
const synthesizeSpeechStep = createStep({
  id: 'synthesize-speech',
  inputSchema: z.object({ narration: z.string() }),
  outputSchema: z.object({
    audioBase64: z.string(),
    mimeType: z.string(),
  }),
  execute: async ({ inputData }) => {
    const { narration } = inputData;

    console.log('ðŸ”Š Synthesizing speech for:', narration);

    // OpenAI TTS API
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'tts-1',
        voice: 'alloy',
        input: narration,
        response_format: 'mp3',
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`TTS API failed (${response.status}): ${errorText}`);
    }

    const audioBuffer = await response.arrayBuffer();
    const audioBase64 = Buffer.from(audioBuffer).toString('base64');

    console.log('âœ… Audio synthesized, size:', audioBase64.length);

    return {
      audioBase64,
      mimeType: 'audio/mpeg',
    };
  },
});

// Workflowå®šç¾©ï¼ˆæœ€æ–°APIï¼‰
export const narrationWorkflow = createWorkflow({
  id: 'narration-workflow',
  inputSchema: z.object({
    cameraImageBase64: z.string(),
    screenImages: z.array(z.string()),
  }),
  outputSchema: z.object({
    audioBase64: z.string(),
    mimeType: z.string(),
  }),
})
  .then(validateImageStep)
  .then(analyzeImageStep)
  .then(generateNarrationStep)
  .then(synthesizeSpeechStep)
  .commit();
